{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dwU8WoEYJcw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "\n",
        "# Variantes de \"de\"\n",
        "PREPOSITIONS = [\n",
        "    r\"\\bde la\\b\",\n",
        "    r\"\\bde\\b\",\n",
        "    r\"\\bdu\\b\",\n",
        "    r\"\\bdes\\b\",\n",
        "    r\"\\bd’\\b\",      # apostrophe typographique\n",
        "    r\"\\bd'\\b\",      # apostrophe simple\n",
        "    r\"\\bd’un\\b\",\n",
        "    r\"\\bd'une\\b\",\n",
        "    r\"\\bde l’\\b\",\n",
        "    r\"\\bde l'\\b\",\n",
        "]\n",
        "\n",
        "# Pattern pour \"N de N\" (avec éventuellement un champ après un \"|\", qu'on ignore ici)\n",
        "pattern = re.compile(\n",
        "    rf\"(.+?)\\s+(?:{'|'.join(PREPOSITIONS)})\\s+(.+?)(?:\\s*\\|\\s*(.*))?$\",\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "\n",
        "def nettoyer_texte(s: str) -> str:\n",
        "    \"\"\"Nettoyage de base : trim, minuscules, accents, ponctuation, espaces multiples.\"\"\"\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = s.strip().lower()\n",
        "\n",
        "    # Normaliser les accents\n",
        "    s = unicodedata.normalize('NFKD', s)\n",
        "    s = ''.join(c for c in s if not unicodedata.combining(c))\n",
        "\n",
        "    # Retirer ponctuation\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
        "    # Retirer chiffres (si non pertinents)\n",
        "    s = re.sub(r\"\\d+\", \"\", s)\n",
        "    # Espaces multiples → un seul\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "def extraire_paires(ligne: str, relation_label: str):\n",
        "    \"\"\"\n",
        "    Extrait (mot1, mot2) à partir d'une ligne de type `N de N ...`\n",
        "    et assigne la relation passée en argument (un seul type de relation par fichier).\n",
        "    \"\"\"\n",
        "    m = pattern.match(ligne)\n",
        "    if not m:\n",
        "        return None\n",
        "    mot1_raw, mot2_raw, _ = m.groups()\n",
        "\n",
        "    mot1 = nettoyer_texte(mot1_raw)\n",
        "    mot2 = nettoyer_texte(mot2_raw)\n",
        "\n",
        "    if not mot1 or not mot2:\n",
        "        return None\n",
        "\n",
        "    return mot1, mot2, relation_label\n",
        "\n",
        "\n",
        "def traiter_fichier(input_path: str, relation_label: str):\n",
        "    \"\"\"\n",
        "    Lit un fichier texte, extrait toutes les paires (mot1, mot2, relation_label)\n",
        "    et renvoie une liste de dict pour ce fichier.\n",
        "    \"\"\"\n",
        "    resultats = []\n",
        "    with open(input_path, encoding=\"utf-8\") as f:\n",
        "        for lineno, ligne in enumerate(f, start=1):\n",
        "            ligne = ligne.strip()\n",
        "            if not ligne:\n",
        "                continue\n",
        "            paire = extraire_paires(ligne, relation_label)\n",
        "            if paire:\n",
        "                mot1, mot2, relation = paire\n",
        "                resultats.append({\"mot1\": mot1, \"mot2\": mot2, \"relation\": relation})\n",
        "            else:\n",
        "                # Tu peux décommenter si tu veux voir les lignes non reconnues\n",
        "                # print(f\"Ligne {lineno} non extraite dans {input_path}: {ligne}\")\n",
        "                pass\n",
        "    return resultats\n",
        "\n",
        "\n",
        "def traiter_dossier(dossier_txt: str,\n",
        "                    output_csv_global: str,\n",
        "                    creer_csv_par_fichier: bool = False):\n",
        "    \"\"\"\n",
        "    Parcourt tous les *.txt du dossier, utilise le nom de fichier comme label de relation\n",
        "    (ex. r_loc_in.txt -> relation = 'r_loc_in'), agrège tout dans un CSV global.\n",
        "    Optionnel : un CSV par fichier.\n",
        "    \"\"\"\n",
        "    toutes_paires = []\n",
        "\n",
        "    for chemin in Path(dossier_txt).glob(\"*.txt\"):\n",
        "        relation_label = chemin.stem  # ex. 'r_loc_in', 'r_depict', etc.\n",
        "\n",
        "        paires = traiter_fichier(str(chemin), relation_label)\n",
        "        print(f\"{chemin.name}: {len(paires)} paires extraites.\")\n",
        "\n",
        "        if creer_csv_par_fichier:\n",
        "            df_f = pd.DataFrame(paires)\n",
        "            df_f = df_f.drop_duplicates(subset=[\"mot1\", \"mot2\", \"relation\"]).reset_index(drop=True)\n",
        "            df_f.to_csv(f\"{chemin.stem}.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "        toutes_paires.extend(paires)\n",
        "\n",
        "    # CSV global\n",
        "    df_global = pd.DataFrame(toutes_paires)\n",
        "    df_global = df_global.drop_duplicates(subset=[\"mot1\", \"mot2\", \"relation\"]).reset_index(drop=True)\n",
        "    df_global.to_csv(output_csv_global, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Total: {len(df_global)} paires uniques écrites dans {output_csv_global}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # À adapter : chemin du dossier où se trouvent tes r_depict.txt, r_has_causitif.txt, etc.\n",
        "    DOSSIER_RELATIONS = \"/content/sample_data/\"  # <-- change ici\n",
        "    SORTIE_GLOBALE = \"base_relations.csv\"\n",
        "\n",
        "    traiter_dossier(\n",
        "        dossier_txt=DOSSIER_RELATIONS,\n",
        "        output_csv_global=SORTIE_GLOBALE,\n",
        "        creer_csv_par_fichier=True  # mets False si tu ne veux QUE le CSV global\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX3r7PAiLFPw",
        "outputId": "74ed2ad9-0b54-4579-d1d0-b1d935fb1fdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r_depict.txt: 1045 paires extraites.\n",
            "r_quantificateur.txt: 880 paires extraites.\n",
            "r_processusinstr-1.txt: 678 paires extraites.\n",
            "r_topic.txt: 996 paires extraites.\n",
            "r_lieu_origine.txt: 438 paires extraites.\n",
            "r_processusagent.txt: 870 paires extraites.\n",
            "r_holo.txt: 868 paires extraites.\n",
            "r_object_matière.txt: 1283 paires extraites.\n",
            "r_has_causitif.txt: 961 paires extraites.\n",
            "r_processuspatient.txt: 1207 paires extraites.\n",
            "r_own-1.txt: 1074 paires extraites.\n",
            "r_social_tie.txt: 803 paires extraites.\n",
            "r_has_property.txt: 1021 paires extraites.\n",
            "r_product_of.txt: 1188 paires extraites.\n",
            "Total: 9885 paires uniques écrites dans base_relations.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfih58J6LsFF",
        "outputId": "13cac530-1336-4f22-d60c-f6c4ad8e282b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4a9pKoZdxRNO"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 0. IMPORTS ET CONFIG\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from numpy.linalg import norm\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1. CHARGER LA BASE GLOBALE\n",
        "# =========================\n",
        "\n",
        "CSV_PATH = \"base_relations.csv\"  # adapter si besoin\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Aperçu des données :\")\n",
        "print(df.head())\n",
        "\n",
        "relations_uniques = sorted(df[\"relation\"].unique())\n",
        "print(\"\\nRelations uniques :\", relations_uniques)\n",
        "print(\"Nombre total de paires :\", len(df))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZUkTlfQLwR3",
        "outputId": "2dd1cd5e-937a-4990-bc58-3de2d509b6aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aperçu des données :\n",
            "       mot1      mot2  relation\n",
            "0     photo   famille  r_depict\n",
            "1  peinture   paysage  r_depict\n",
            "2   gravure  monument  r_depict\n",
            "3   fresque  bataille  r_depict\n",
            "4  mosaique      dieu  r_depict\n",
            "\n",
            "Relations uniques : ['r_depict', 'r_has_causitif', 'r_has_property', 'r_holo', 'r_lieu_origine', 'r_object_matière', 'r_own-1', 'r_processusagent', 'r_processusinstr-1', 'r_processuspatient', 'r_product_of', 'r_quantificateur', 'r_social_tie', 'r_topic']\n",
            "Nombre total de paires : 9885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 2. FONCTIONS UTILITAIRES\n",
        "# =========================\n",
        "\n",
        "def get_vec(word, model, dim):\n",
        "    \"\"\"\n",
        "    Retourne le vecteur Word2Vec du mot s'il existe, sinon un vecteur nul.\n",
        "    \"\"\"\n",
        "    if word in model.wv:\n",
        "        return model.wv[word]\n",
        "    else:\n",
        "        return np.zeros(dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "def combine(v1, v2):\n",
        "    \"\"\"\n",
        "    Combine deux vecteurs v1, v2 en un vecteur de paire :\n",
        "    [v1, v2, |v1 - v2|, v1 * v2]\n",
        "    \"\"\"\n",
        "    v1 = np.asarray(v1, dtype=np.float32)\n",
        "    v2 = np.asarray(v2, dtype=np.float32)\n",
        "    diff = np.abs(v1 - v2)\n",
        "    prod = v1 * v2\n",
        "    return np.concatenate([v1, v2, diff, prod], axis=-1)\n",
        "\n",
        "\n",
        "def encode_pair(mot1, mot2, w2v_model, emb_dim):\n",
        "    \"\"\"\n",
        "    Encode un couple (mot1, mot2) en vecteur combiné avec un modèle Word2Vec donné.\n",
        "    \"\"\"\n",
        "    v1 = get_vec(mot1, w2v_model, emb_dim)\n",
        "    v2 = get_vec(mot2, w2v_model, emb_dim)\n",
        "    return combine(v1, v2)\n"
      ],
      "metadata": {
        "id": "VTHorxJ9L_fl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 3. ENTRAÎNER LES MODÈLES PAR RELATION\n",
        "# =========================\n",
        "\n",
        "emb_dim = 100     # dimension des embeddings Word2Vec\n",
        "neg_ratio = 1.0   # négatifs ≈ positifs (1.0 = autant de négatifs que de positifs)\n",
        "\n",
        "models_R = {}     # relation -> {\"w2v\": w2v_R, \"tree\": tree_R, \"emb_dim\": emb_dim}\n",
        "\n",
        "for R in relations_uniques:\n",
        "    print(f\"\\n=== Relation {R} ===\")\n",
        "\n",
        "    # 3.1 POSITIFS : toutes les paires avec relation R\n",
        "    df_pos = df[df[\"relation\"] == R].reset_index(drop=True)\n",
        "    n_pos = len(df_pos)\n",
        "    print(f\"Nb positifs (R) :\", n_pos)\n",
        "\n",
        "    if n_pos == 0:\n",
        "        print(\"Aucun positif pour cette relation, on saute.\")\n",
        "        continue\n",
        "\n",
        "    # 3.2 NÉGATIFS : paires avec relation != R (échantillon)\n",
        "    df_neg_all = df[df[\"relation\"] != R]\n",
        "    n_neg_cible = int(neg_ratio * n_pos)\n",
        "    n_neg_cible = min(n_neg_cible, len(df_neg_all))\n",
        "\n",
        "    df_neg = df_neg_all.sample(n=n_neg_cible, random_state=42).reset_index(drop=True)\n",
        "    print(f\"Nb négatifs (pas R) :\", len(df_neg))\n",
        "\n",
        "    # 3.3 Corpus pour Word2Vec_R : uniquement les positifs (mot1, mot2) de R\n",
        "    sentences_R = df_pos[[\"mot1\", \"mot2\"]].values.tolist()\n",
        "\n",
        "    w2v_R = Word2Vec(\n",
        "        sentences=sentences_R,\n",
        "        vector_size=emb_dim,\n",
        "        window=5,\n",
        "        min_count=1,\n",
        "        workers=4,\n",
        "        sg=1\n",
        "    )\n",
        "\n",
        "    # 3.4 Construire les vecteurs combinés POSITIFS\n",
        "    X_pos = []\n",
        "    for _, row in df_pos.iterrows():\n",
        "        v_pair = encode_pair(row[\"mot1\"], row[\"mot2\"], w2v_R, emb_dim)\n",
        "        X_pos.append(v_pair)\n",
        "    X_pos = np.stack(X_pos)\n",
        "    y_pos = np.ones(len(X_pos), dtype=np.int64)   # label 1 = \"est R\"\n",
        "\n",
        "    # 3.5 Construire les vecteurs combinés NÉGATIFS\n",
        "    X_neg = []\n",
        "    for _, row in df_neg.iterrows():\n",
        "        v_pair = encode_pair(row[\"mot1\"], row[\"mot2\"], w2v_R, emb_dim)\n",
        "        X_neg.append(v_pair)\n",
        "    X_neg = np.stack(X_neg)\n",
        "    y_neg = np.zeros(len(X_neg), dtype=np.int64)  # label 0 = \"pas R\"\n",
        "\n",
        "    # 3.6 Dataset final pour la relation R\n",
        "    X_R = np.vstack([X_pos, X_neg])\n",
        "    y_R = np.concatenate([y_pos, y_neg])\n",
        "\n",
        "    # 3.7 Split train / val\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_R, y_R, test_size=0.2, stratify=y_R, random_state=42\n",
        "    )\n",
        "\n",
        "    # 3.8 Arbre de décision binaire pour R\n",
        "    tree_R = DecisionTreeClassifier(\n",
        "        max_depth=15,\n",
        "        min_samples_leaf=5,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=42\n",
        "    )\n",
        "    tree_R.fit(X_train, y_train)\n",
        "\n",
        "    val_acc = tree_R.score(X_val, y_val)\n",
        "    print(f\"Accuracy val pour {R} : {val_acc:.3f}\")\n",
        "\n",
        "    # 3.9 Stocker le modèle\n",
        "    models_R[R] = {\n",
        "        \"w2v\": w2v_R,\n",
        "        \"tree\": tree_R,\n",
        "        \"emb_dim\": emb_dim,\n",
        "        \"n_pos\": int(n_pos),\n",
        "        \"n_neg\": int(len(df_neg))\n",
        "    }\n",
        "\n",
        "print(\"\\nNombre de relations effectivement entraînées :\", len(models_R))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz6ajyXvMBzm",
        "outputId": "dd930681-c188-4bac-a9c8-d363ac13728b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Relation r_depict ===\n",
            "Nb positifs (R) : 977\n",
            "Nb négatifs (pas R) : 977\n",
            "Accuracy val pour r_depict : 0.967\n",
            "\n",
            "=== Relation r_has_causitif ===\n",
            "Nb positifs (R) : 739\n",
            "Nb négatifs (pas R) : 739\n",
            "Accuracy val pour r_has_causitif : 1.000\n",
            "\n",
            "=== Relation r_has_property ===\n",
            "Nb positifs (R) : 821\n",
            "Nb négatifs (pas R) : 821\n",
            "Accuracy val pour r_has_property : 0.985\n",
            "\n",
            "=== Relation r_holo ===\n",
            "Nb positifs (R) : 690\n",
            "Nb négatifs (pas R) : 690\n",
            "Accuracy val pour r_holo : 0.996\n",
            "\n",
            "=== Relation r_lieu_origine ===\n",
            "Nb positifs (R) : 365\n",
            "Nb négatifs (pas R) : 365\n",
            "Accuracy val pour r_lieu_origine : 0.959\n",
            "\n",
            "=== Relation r_object_matière ===\n",
            "Nb positifs (R) : 600\n",
            "Nb négatifs (pas R) : 600\n",
            "Accuracy val pour r_object_matière : 0.971\n",
            "\n",
            "=== Relation r_own-1 ===\n",
            "Nb positifs (R) : 388\n",
            "Nb négatifs (pas R) : 388\n",
            "Accuracy val pour r_own-1 : 0.987\n",
            "\n",
            "=== Relation r_processusagent ===\n",
            "Nb positifs (R) : 513\n",
            "Nb négatifs (pas R) : 513\n",
            "Accuracy val pour r_processusagent : 0.995\n",
            "\n",
            "=== Relation r_processusinstr-1 ===\n",
            "Nb positifs (R) : 592\n",
            "Nb négatifs (pas R) : 592\n",
            "Accuracy val pour r_processusinstr-1 : 0.987\n",
            "\n",
            "=== Relation r_processuspatient ===\n",
            "Nb positifs (R) : 778\n",
            "Nb négatifs (pas R) : 778\n",
            "Accuracy val pour r_processuspatient : 0.971\n",
            "\n",
            "=== Relation r_product_of ===\n",
            "Nb positifs (R) : 1049\n",
            "Nb négatifs (pas R) : 1049\n",
            "Accuracy val pour r_product_of : 0.979\n",
            "\n",
            "=== Relation r_quantificateur ===\n",
            "Nb positifs (R) : 864\n",
            "Nb négatifs (pas R) : 864\n",
            "Accuracy val pour r_quantificateur : 0.942\n",
            "\n",
            "=== Relation r_social_tie ===\n",
            "Nb positifs (R) : 699\n",
            "Nb négatifs (pas R) : 699\n",
            "Accuracy val pour r_social_tie : 0.986\n",
            "\n",
            "=== Relation r_topic ===\n",
            "Nb positifs (R) : 810\n",
            "Nb négatifs (pas R) : 810\n",
            "Accuracy val pour r_topic : 0.960\n",
            "\n",
            "Nombre de relations effectivement entraînées : 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 4. SAUVEGARDE DES MODÈLES\n",
        "# =========================\n",
        "\n",
        "MODEL_DIR = \"models_relations\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# 4.1 Sauvegarde de chaque modèle par relation\n",
        "for R, obj in models_R.items():\n",
        "    w2v_R = obj[\"w2v\"]\n",
        "    tree_R = obj[\"tree\"]\n",
        "    emb_dim_R = obj[\"emb_dim\"]\n",
        "\n",
        "    w2v_path = os.path.join(MODEL_DIR, f\"w2v_{R}.model\")\n",
        "    tree_path = os.path.join(MODEL_DIR, f\"tree_{R}.pkl\")\n",
        "\n",
        "    # Word2Vec\n",
        "    w2v_R.save(w2v_path)\n",
        "\n",
        "    # arbre de décision\n",
        "    with open(tree_path, \"wb\") as f:\n",
        "        pickle.dump(tree_R, f)\n",
        "\n",
        "    print(f\"Sauvé : {w2v_path} et {tree_path}\")\n",
        "\n",
        "# 4.2 Sauvegarde des métadonnées globales\n",
        "metadata = {\n",
        "    \"relations\": list(models_R.keys()),\n",
        "    \"emb_dim\": emb_dim\n",
        "}\n",
        "\n",
        "meta_path = os.path.join(MODEL_DIR, \"metadata.json\")\n",
        "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\nMétadonnées sauvegardées dans {meta_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arVoEo8HMFm4",
        "outputId": "d5dbfc11-c777-4855-f726-623347c9b17a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sauvé : models_relations/w2v_r_depict.model et models_relations/tree_r_depict.pkl\n",
            "Sauvé : models_relations/w2v_r_has_causitif.model et models_relations/tree_r_has_causitif.pkl\n",
            "Sauvé : models_relations/w2v_r_has_property.model et models_relations/tree_r_has_property.pkl\n",
            "Sauvé : models_relations/w2v_r_holo.model et models_relations/tree_r_holo.pkl\n",
            "Sauvé : models_relations/w2v_r_lieu_origine.model et models_relations/tree_r_lieu_origine.pkl\n",
            "Sauvé : models_relations/w2v_r_object_matière.model et models_relations/tree_r_object_matière.pkl\n",
            "Sauvé : models_relations/w2v_r_own-1.model et models_relations/tree_r_own-1.pkl\n",
            "Sauvé : models_relations/w2v_r_processusagent.model et models_relations/tree_r_processusagent.pkl\n",
            "Sauvé : models_relations/w2v_r_processusinstr-1.model et models_relations/tree_r_processusinstr-1.pkl\n",
            "Sauvé : models_relations/w2v_r_processuspatient.model et models_relations/tree_r_processuspatient.pkl\n",
            "Sauvé : models_relations/w2v_r_product_of.model et models_relations/tree_r_product_of.pkl\n",
            "Sauvé : models_relations/w2v_r_quantificateur.model et models_relations/tree_r_quantificateur.pkl\n",
            "Sauvé : models_relations/w2v_r_social_tie.model et models_relations/tree_r_social_tie.pkl\n",
            "Sauvé : models_relations/w2v_r_topic.model et models_relations/tree_r_topic.pkl\n",
            "\n",
            "Métadonnées sauvegardées dans models_relations/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 5. RECHARGEMENT DES MODÈLES\n",
        "# =========================\n",
        "\n",
        "MODEL_DIR = \"models_relations\"\n",
        "\n",
        "# 5.1 Charger les métadonnées\n",
        "meta_path = os.path.join(MODEL_DIR, \"metadata.json\")\n",
        "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "relations_loaded = metadata[\"relations\"]\n",
        "emb_dim_loaded = metadata[\"emb_dim\"]\n",
        "\n",
        "print(\"Relations chargées :\", relations_loaded)\n",
        "print(\"Dimension d'embedding :\", emb_dim_loaded)\n",
        "\n",
        "# 5.2 Reconstruire models_R_loaded\n",
        "models_R_loaded = {}\n",
        "\n",
        "for R in relations_loaded:\n",
        "    w2v_path = os.path.join(MODEL_DIR, f\"w2v_{R}.model\")\n",
        "    tree_path = os.path.join(MODEL_DIR, f\"tree_{R}.pkl\")\n",
        "\n",
        "    # Word2Vec_R\n",
        "    w2v_R = Word2Vec.load(w2v_path)\n",
        "\n",
        "    # arbre R\n",
        "    with open(tree_path, \"rb\") as f:\n",
        "        tree_R = pickle.load(f)\n",
        "\n",
        "    models_R_loaded[R] = {\n",
        "        \"w2v\": w2v_R,\n",
        "        \"tree\": tree_R,\n",
        "        \"emb_dim\": emb_dim_loaded\n",
        "    }\n",
        "\n",
        "print(\"Nombre de modèles rechargés :\", len(models_R_loaded))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZogVMiPMMPN",
        "outputId": "4b3011da-6d96-4290-82e6-016c1b6383f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relations chargées : ['r_depict', 'r_has_causitif', 'r_has_property', 'r_holo', 'r_lieu_origine', 'r_object_matière', 'r_own-1', 'r_processusagent', 'r_processusinstr-1', 'r_processuspatient', 'r_product_of', 'r_quantificateur', 'r_social_tie', 'r_topic']\n",
            "Dimension d'embedding : 100\n",
            "Nombre de modèles rechargés : 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 6. PRÉDICTION POUR UNE NOUVELLE PAIRE\n",
        "# =========================\n",
        "\n",
        "def predict_relation_multi_R(mot1, mot2, models_R, seuil=0.0):\n",
        "    \"\"\"\n",
        "    Prédit la relation pour (mot1, mot2) en testant tous les modèles par relation.\n",
        "\n",
        "    mot1, mot2 : strings\n",
        "    models_R : dict[relation] -> {\"w2v\": w2v_R, \"tree\": tree_R, \"emb_dim\": emb_dim}\n",
        "    seuil : seuil minimal de probabilité pour accepter une relation.\n",
        "\n",
        "    Retourne :\n",
        "      - relation_finale (str ou None si rejet)\n",
        "      - score_max (float)\n",
        "      - scores_par_relation (dict[relation] -> score)\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "\n",
        "    for R, obj in models_R.items():\n",
        "        w2v_R = obj[\"w2v\"]\n",
        "        tree_R = obj[\"tree\"]\n",
        "        emb_dim = obj[\"emb_dim\"]\n",
        "\n",
        "        v_pair = encode_pair(mot1, mot2, w2v_R, emb_dim).reshape(1, -1)\n",
        "\n",
        "        if hasattr(tree_R, \"predict_proba\"):\n",
        "            proba = tree_R.predict_proba(v_pair)[0, 1]  # proba classe 1 = \"est R\"\n",
        "        else:\n",
        "            pred = tree_R.predict(v_pair)[0]\n",
        "            proba = float(pred)\n",
        "\n",
        "        scores[R] = float(proba)\n",
        "\n",
        "    # relation avec probabilité max\n",
        "    relation_max = max(scores, key=scores.get)\n",
        "    score_max = scores[relation_max]\n",
        "\n",
        "    if score_max < seuil:\n",
        "        # si aucune relation n'est jugée suffisamment probable\n",
        "        return None, score_max, scores\n",
        "\n",
        "    return relation_max, score_max, scores\n"
      ],
      "metadata": {
        "id": "7-s7y7CXMPRi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de de\n",
        "mot1_test = \"desert\"\n",
        "mot2_test = \"frankfurt\"\n",
        "\n",
        "relation_predite, score_max, scores_detail = predict_relation_multi_R(\n",
        "    mot1_test,\n",
        "    mot2_test,\n",
        "    models_R_loaded,   # ou models_R si tu es encore dans le même notebook\n",
        "    seuil=0.0          # tu peux mettre 0.4 ou 0.5 pour être plus strict\n",
        ")\n",
        "\n",
        "print(\"Mot1 :\", mot1_test)\n",
        "print(\"Mot2 :\", mot2_test)\n",
        "print(\"Relation prédite :\", relation_predite)\n",
        "print(\"Score max :\", score_max)\n",
        "print(\"Scores par relation :\")\n",
        "for r, s in scores_detail.items():\n",
        "    print(f\"  {r}: {s:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPj5AomIMR3_",
        "outputId": "579659bf-0053-4a33-b865-316eef4b027e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mot1 : desert\n",
            "Mot2 : frankfurt\n",
            "Relation prédite : r_social_tie\n",
            "Score max : 0.0021008403361344537\n",
            "Scores par relation :\n",
            "  r_depict: 0.000\n",
            "  r_has_causitif: 0.000\n",
            "  r_has_property: 0.000\n",
            "  r_holo: 0.000\n",
            "  r_lieu_origine: 0.000\n",
            "  r_object_matière: 0.000\n",
            "  r_own-1: 0.000\n",
            "  r_processusagent: 0.000\n",
            "  r_processusinstr-1: 0.000\n",
            "  r_processuspatient: 0.000\n",
            "  r_product_of: 0.000\n",
            "  r_quantificateur: 0.000\n",
            "  r_social_tie: 0.002\n",
            "  r_topic: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vcg49OKCMTvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}