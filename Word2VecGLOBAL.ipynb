{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "En résumé ultra-court\n",
        "\n",
        "Word2Vec encode chaque mot en vecteur dense.On entraîne un seul modèle Word2Vec sur toutes les paires (mot1, mot2) de ton fichier base_relations.csv.\n",
        "\n",
        "Pour chaque couple (mot1, mot2), on crée un vecteur combiné [v1, v2, |v1−v2|, v1*v2].\n",
        "\n",
        "On entraîne un arbre de décision pour classifier ce vecteur en une relation.\n",
        "\n",
        "Pour un nouveau couple, on encode + combine → on prédit la relation avec l’arbre."
      ],
      "metadata": {
        "id": "S6xsjCGcDHCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "\n",
        "# Variantes de \"de\"\n",
        "PREPOSITIONS = [\n",
        "    r\"\\bde la\\b\",\n",
        "    r\"\\bde\\b\",\n",
        "    r\"\\bdu\\b\",\n",
        "    r\"\\bdes\\b\",\n",
        "    r\"\\bd’\\b\",      # apostrophe typographique\n",
        "    r\"\\bd'\\b\",      # apostrophe simple\n",
        "    r\"\\bd’un\\b\",\n",
        "    r\"\\bd'une\\b\",\n",
        "    r\"\\bde l’\\b\",\n",
        "    r\"\\bde l'\\b\",\n",
        "]\n",
        "\n",
        "# Pattern pour \"N de N\" (avec éventuellement un champ après un \"|\", qu'on ignore ici)\n",
        "pattern = re.compile(\n",
        "    rf\"(.+?)\\s+(?:{'|'.join(PREPOSITIONS)})\\s+(.+?)(?:\\s*\\|\\s*(.*))?$\",\n",
        "    flags=re.IGNORECASE\n",
        ")\n",
        "\n",
        "\n",
        "def nettoyer_texte(s: str) -> str:\n",
        "    \"\"\"Nettoyage de base : trim, minuscules, accents, ponctuation, espaces multiples.\"\"\"\n",
        "    if s is None:\n",
        "        return None\n",
        "    s = s.strip().lower()\n",
        "\n",
        "    # Normaliser les accents\n",
        "    s = unicodedata.normalize('NFKD', s)\n",
        "    s = ''.join(c for c in s if not unicodedata.combining(c))\n",
        "\n",
        "    # Retirer ponctuation\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
        "    # Retirer chiffres (si non pertinents)\n",
        "    s = re.sub(r\"\\d+\", \"\", s)\n",
        "    # Espaces multiples → un seul\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "def extraire_paires(ligne: str, relation_label: str):\n",
        "    \"\"\"\n",
        "    Extrait (mot1, mot2) à partir d'une ligne de type `N de N ...`\n",
        "    et assigne la relation passée en argument (un seul type de relation par fichier).\n",
        "    \"\"\"\n",
        "    m = pattern.match(ligne)\n",
        "    if not m:\n",
        "        return None\n",
        "    mot1_raw, mot2_raw, _ = m.groups()\n",
        "\n",
        "    mot1 = nettoyer_texte(mot1_raw)\n",
        "    mot2 = nettoyer_texte(mot2_raw)\n",
        "\n",
        "    if not mot1 or not mot2:\n",
        "        return None\n",
        "\n",
        "    return mot1, mot2, relation_label\n",
        "\n",
        "\n",
        "def traiter_fichier(input_path: str, relation_label: str):\n",
        "    \"\"\"\n",
        "    Lit un fichier texte, extrait toutes les paires (mot1, mot2, relation_label)\n",
        "    et renvoie une liste de dict pour ce fichier.\n",
        "    \"\"\"\n",
        "    resultats = []\n",
        "    with open(input_path, encoding=\"utf-8\") as f:\n",
        "        for lineno, ligne in enumerate(f, start=1):\n",
        "            ligne = ligne.strip()\n",
        "            if not ligne:\n",
        "                continue\n",
        "            paire = extraire_paires(ligne, relation_label)\n",
        "            if paire:\n",
        "                mot1, mot2, relation = paire\n",
        "                resultats.append({\"mot1\": mot1, \"mot2\": mot2, \"relation\": relation})\n",
        "            else:\n",
        "                # Tu peux décommenter si tu veux voir les lignes non reconnues\n",
        "                # print(f\"Ligne {lineno} non extraite dans {input_path}: {ligne}\")\n",
        "                pass\n",
        "    return resultats\n",
        "\n",
        "\n",
        "def traiter_dossier(dossier_txt: str,\n",
        "                    output_csv_global: str,\n",
        "                    creer_csv_par_fichier: bool = False):\n",
        "    \"\"\"\n",
        "    Parcourt tous les *.txt du dossier, utilise le nom de fichier comme label de relation\n",
        "    (ex. r_loc_in.txt -> relation = 'r_loc_in'), agrège tout dans un CSV global.\n",
        "    Optionnel : un CSV par fichier.\n",
        "    \"\"\"\n",
        "    toutes_paires = []\n",
        "\n",
        "    for chemin in Path(dossier_txt).glob(\"*.txt\"):\n",
        "        relation_label = chemin.stem  # ex. 'r_loc_in', 'r_depict', etc.\n",
        "\n",
        "        paires = traiter_fichier(str(chemin), relation_label)\n",
        "        print(f\"{chemin.name}: {len(paires)} paires extraites.\")\n",
        "\n",
        "        if creer_csv_par_fichier:\n",
        "            df_f = pd.DataFrame(paires)\n",
        "            df_f = df_f.drop_duplicates(subset=[\"mot1\", \"mot2\", \"relation\"]).reset_index(drop=True)\n",
        "            df_f.to_csv(f\"{chemin.stem}.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "        toutes_paires.extend(paires)\n",
        "\n",
        "    # CSV global\n",
        "    df_global = pd.DataFrame(toutes_paires)\n",
        "    df_global = df_global.drop_duplicates(subset=[\"mot1\", \"mot2\", \"relation\"]).reset_index(drop=True)\n",
        "    df_global.to_csv(output_csv_global, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Total: {len(df_global)} paires uniques écrites dans {output_csv_global}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # À adapter : chemin du dossier où se trouvent tes r_depict.txt, r_has_causitif.txt, etc.\n",
        "    DOSSIER_RELATIONS = \"/content/sample_data/\"  # <-- change ici\n",
        "    SORTIE_GLOBALE = \"base_relations.csv\"\n",
        "\n",
        "    traiter_dossier(\n",
        "        dossier_txt=DOSSIER_RELATIONS,\n",
        "        output_csv_global=SORTIE_GLOBALE,\n",
        "        creer_csv_par_fichier=True  # mets False si tu ne veux QUE le CSV global\n",
        "    )\n"
      ],
      "metadata": {
        "id": "nkAyNTJzI109",
        "outputId": "112d349b-6581-4fed-8139-1ce3e9a648ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r_depict.txt: 1045 paires extraites.\n",
            "r_quantificateur.txt: 880 paires extraites.\n",
            "r_processusinstr-1.txt: 678 paires extraites.\n",
            "r_topic.txt: 996 paires extraites.\n",
            "r_lieu_origine.txt: 438 paires extraites.\n",
            "r_processusagent.txt: 870 paires extraites.\n",
            "r_holo.txt: 868 paires extraites.\n",
            "r_object_matière.txt: 1283 paires extraites.\n",
            "r_has_causitif.txt: 961 paires extraites.\n",
            "r_processuspatient.txt: 1207 paires extraites.\n",
            "r_own-1.txt: 1074 paires extraites.\n",
            "r_social_tie.txt: 803 paires extraites.\n",
            "r_has_property.txt: 1021 paires extraites.\n",
            "r_product_of.txt: 1188 paires extraites.\n",
            "Total: 9885 paires uniques écrites dans base_relations.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim"
      ],
      "metadata": {
        "id": "pQOPKv2F_MRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "df = pd.read_csv(\"base_relations.csv\")  # colonnes: mot1, mot2, relation\n",
        "\n",
        "# entraînement / chargement Word2Vec\n",
        "emb_dim = 100\n",
        "sentences = df[[\"mot1\", \"mot2\"]].values.tolist()\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=emb_dim,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    sg=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z6wU7x3435GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vec(word, model, dim):\n",
        "    if word in model.wv:\n",
        "        return model.wv[word]\n",
        "    else:\n",
        "        return np.zeros(dim, dtype=np.float32)\n",
        "\n",
        "def combine(v1, v2):\n",
        "    v1 = np.asarray(v1, dtype=np.float32)\n",
        "    v2 = np.asarray(v2, dtype=np.float32)\n",
        "    diff = np.abs(v1 - v2)\n",
        "    prod = v1 * v2\n",
        "    return np.concatenate([v1, v2, diff, prod], axis=-1)  # dim = 4 * emb_dim\n"
      ],
      "metadata": {
        "id": "xteH75ue_Iim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = []\n",
        "for _, row in df.iterrows():\n",
        "    v1 = get_vec(row[\"mot1\"], w2v_model, emb_dim)\n",
        "    v2 = get_vec(row[\"mot2\"], w2v_model, emb_dim)\n",
        "    X.append(combine(v1, v2))\n",
        "\n",
        "X = np.stack(X)  # shape (N, 4*emb_dim)\n",
        "\n",
        "le_rel = LabelEncoder()\n",
        "y = le_rel.fit_transform(df[\"relation\"])  # entiers 0..C-1\n"
      ],
      "metadata": {
        "id": "iWlT0LFJ_qRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# split train / test (tu peux ajouter un val si besoin)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# arbre de décision\n",
        "tree_clf = DecisionTreeClassifier(\n",
        "    max_depth=20,          # à ajuster\n",
        "    min_samples_leaf=5,    # évite le sur-apprentissage\n",
        "    class_weight=\"balanced\",  # utile si les classes sont déséquilibrées\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# évaluation\n",
        "y_pred = tree_clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=le_rel.classes_))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFJqxseB_se4",
        "outputId": "b4230059-ff6a-495e-8903-1f4009c93384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "          r_depict       0.00      0.00      0.00       195\n",
            "    r_has_causitif       0.08      0.98      0.15       148\n",
            "    r_has_property       0.00      0.00      0.00       164\n",
            "            r_holo       0.00      0.00      0.00       138\n",
            "    r_lieu_origine       0.33      0.03      0.05        73\n",
            "  r_object_matière       0.83      0.37      0.51       120\n",
            "           r_own-1       0.92      0.29      0.44        77\n",
            "  r_processusagent       0.17      0.02      0.03       103\n",
            "r_processusinstr-1       0.25      0.02      0.03       118\n",
            "r_processuspatient       0.78      0.04      0.08       156\n",
            "      r_product_of       0.97      0.16      0.28       210\n",
            "  r_quantificateur       0.97      0.35      0.52       173\n",
            "      r_social_tie       0.20      0.01      0.01       140\n",
            "           r_topic       0.83      0.18      0.29       162\n",
            "\n",
            "          accuracy                           0.18      1977\n",
            "         macro avg       0.45      0.17      0.17      1977\n",
            "      weighted avg       0.46      0.18      0.17      1977\n",
            "\n",
            "[[  0 186   0   1   0   2   1   1   3   0   0   1   0   0]\n",
            " [  0 145   2   0   0   0   0   0   0   0   0   0   1   0]\n",
            " [  0 162   0   0   0   2   0   0   0   0   0   0   0   0]\n",
            " [  0 131   0   0   1   1   0   0   3   0   1   0   0   1]\n",
            " [  0  69   0   0   2   0   0   0   0   0   0   1   1   0]\n",
            " [  0  76   0   0   0  44   0   0   0   0   0   0   0   0]\n",
            " [  0  54   0   0   0   0  22   0   0   0   0   0   1   0]\n",
            " [  0  97   0   0   0   0   0   2   0   0   0   0   0   4]\n",
            " [  0 112   0   0   1   3   0   0   2   0   0   0   0   0]\n",
            " [  0 147   0   0   0   0   0   0   0   7   0   0   1   1]\n",
            " [  0 176   0   0   0   0   0   0   0   0  34   0   0   0]\n",
            " [  0 108   0   1   1   1   0   0   0   1   0  61   0   0]\n",
            " [  0 137   0   0   1   0   1   0   0   0   0   0   1   0]\n",
            " [  0 123   0   0   0   0   0   9   0   1   0   0   0  29]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_relation_tree(mot1, mot2, model_w2v, emb_dim, tree_clf, label_encoder):\n",
        "    v1 = get_vec(mot1, model_w2v, emb_dim)\n",
        "    v2 = get_vec(mot2, model_w2v, emb_dim)\n",
        "    v_pair = combine(v1, v2).reshape(1, -1)  # (1, 4*emb_dim)\n",
        "\n",
        "    y_pred = tree_clf.predict(v_pair)[0]\n",
        "    proba = tree_clf.predict_proba(v_pair)[0].max()  # probabilité de la classe prédite\n",
        "    relation = label_encoder.inverse_transform([y_pred])[0]\n",
        "    return relation, float(proba)\n",
        "\n",
        "# Exemple d'utilisation\n",
        "relation, proba = predict_relation_tree(\n",
        "    \"chien\", \"animal\",\n",
        "    w2v_model, emb_dim,\n",
        "    tree_clf, le_rel\n",
        ")\n",
        "print(\"relation prédite :\", relation, \"confiance :\", proba)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdz_nUgU_1Iw",
        "outputId": "7f011fe3-d1ee-4484-cc26-e8dcc78b9c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relation prédite : r_has_causitif confiance : 0.0810050555562672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgnxipAfAbsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}